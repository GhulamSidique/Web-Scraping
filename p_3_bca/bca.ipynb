{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(path):\n",
    "    # to avoid windows to close again and again we make use of headless\n",
    "    options = ChromeOptions()\n",
    "    options.headless=True\n",
    "    # make use of chrome for scraping\n",
    "    service = Service(r\"E:\\web_scarping\\project_3\\chromedriver.exe\")\n",
    "    # create a driver using chrome\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    # run the driver through url\n",
    "    driver.get(path)\n",
    "\n",
    "    # details\n",
    "    # details = {}\n",
    "    results = []\n",
    "\n",
    "    # =============================================================================================\n",
    "    # complete login\n",
    "\n",
    "    try:\n",
    "        provided_u_name = \"haider1805@icloud.com\"\n",
    "        user_name_id = \"username\"\n",
    "        user_name = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.NAME, user_name_id)))   \n",
    "        user_name.send_keys(provided_u_name)\n",
    "    except Exception as e:\n",
    "        print(\"No username tab found\")\n",
    "    # get password tab\n",
    "    try:\n",
    "        provided_pass = \"Muhssan7865\"\n",
    "        pass_id = \"password\"\n",
    "        password = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.NAME, pass_id)))\n",
    "        password.send_keys(provided_pass)\n",
    "    except Exception as e:\n",
    "        print(\"No password tab found\") \n",
    "     # get the login tab to enter the username and password\n",
    "    try:\n",
    "        # login_tab_css = \"submit\"\n",
    "        login_tab = driver.find_element(By.CLASS_NAME,  \"onelogin-main__btn\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", login_tab)\n",
    "        login_tab.click()\n",
    "    except Exception as e:\n",
    "        print(\"No login tab found here\")\n",
    "    \n",
    "    # =====================================================================================  \n",
    "    # get the number of cars found\n",
    "    try:\n",
    "        num_cars_css = \"//p[@data-testid='lot-navigation-number']\" # here we will have number of cars found for the current website\n",
    "        num_cars = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.XPATH, num_cars_css)))\n",
    "        nums = num_cars.text\n",
    "        match = re.search(r\"/(\\d+)\", nums) # beacuse of the date format\n",
    "        total_cars = match.group(1)\n",
    "    except Exception as e:\n",
    "        print(\"Nothing found\")\n",
    "    \n",
    "    # =============================================================================================\n",
    "    # Handle cookie consent banner\n",
    "    try:\n",
    "        # cookie_main_bar = driver.find_element(By.ID, \"onetrust-button-group\")\n",
    "        cookie_accept = driver.find_element(By.ID, \"onetrust-reject-all-handler\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", cookie_accept)\n",
    "        time.sleep(1)\n",
    "        cookie_accept.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling cookie banner: {e}\")\n",
    "\n",
    "    # =====================================================================================  \n",
    "    # # get the next button\n",
    "    # try:\n",
    "    #     # next_button_bar = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, \"OVeuM\")))\n",
    "    #     next_button = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"next-lot-navigation-button\"]')))\n",
    "    #     # driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "    #     # next_button = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.hpQdrD\")))\n",
    "    #     # print(next_button.text)\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(\"No next button here\")\n",
    "\n",
    "    # =====================================================================================  \n",
    "\n",
    "    # run till the number of cars available from total_cars above\n",
    "    for i in range(5): # for sample purpose\n",
    "        details={}\n",
    "        # try:\n",
    "        #     # cookie_main_bar = driver.find_element(By.ID, \"onetrust-button-group\")\n",
    "        #     cookie_accept = driver.find_element(By.ID, \"onetrust-reject-all-handler\")\n",
    "        #     driver.execute_script(\"arguments[0].scrollIntoView(true);\", cookie_accept)\n",
    "        #     cookie_accept.click()\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error handling cookie banner: {e}\")\n",
    "        # get the car reg and name, mileage, featured in, planned, date & venue, auction name, vehicle location and lot number\n",
    "        try:\n",
    "            container_class = \"VehicleInteraction__VehicleDetailsWrapper-sc-1g915f-0\"\n",
    "            container = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, container_class)))\n",
    "\n",
    "            # get the reg\n",
    "            reg_class = \"bLafSa\"\n",
    "            reg = container.find_element(By.CLASS_NAME, reg_class)\n",
    "\n",
    "            # get the car name\n",
    "            name_css = \"VehicleInteraction___StyledTypography-sc-1g915f-19\"\n",
    "            name = container.find_element(By.CLASS_NAME, name_css)\n",
    "\n",
    "            # get the mileage\n",
    "            mileage = driver.find_element(By.CSS_SELECTOR, \"h6.sc-fqkvVR.iUxEsY\")\n",
    "            mileage_text=mileage.text \n",
    "\n",
    "            # get the featured in as a label\n",
    "            feat_in_l = driver.find_element(By.CSS_SELECTOR, \"p.jLnlNa\").text\n",
    "            # get the value of above\n",
    "            feat_in_v = driver.find_element(By.CSS_SELECTOR, \"h5.sc-fqkvVR.kMeqUD\").text\n",
    "\n",
    "            # get the planned label\n",
    "            planned_l = driver.find_element(By.CSS_SELECTOR, \"span.iXOsDU\").text\n",
    "            planned_v = driver.find_element(By.CSS_SELECTOR, \"p.sc-fqkvVR.jPRetG\").text\n",
    "\n",
    "            # get the day-date and venue\n",
    "            date_ven_lst = []\n",
    "            date_ven = driver.find_elements(By.CSS_SELECTOR, \"p.sc-fqkvVR.bnHyrd\")\n",
    "            for v in date_ven:\n",
    "                date_ven_lst.append(v.text)\n",
    "\n",
    "            # get auction name\n",
    "            auction_name = driver.find_element(By.CSS_SELECTOR, \"h6.EeoBD\").text\n",
    "            # get location\n",
    "            veh_location = driver.find_element(By.CLASS_NAME, \"fUnarQ\").text\n",
    "            # get lots\n",
    "            lot_num = driver.find_element(By.CLASS_NAME, \"joByeZ\").text\n",
    "\n",
    "\n",
    "            details[\"Reg_No\"] = reg.text\n",
    "            details['Name'] = name.text\n",
    "            details[\"Mileage\"] = mileage_text\n",
    "            details[feat_in_l] = feat_in_v\n",
    "            details[planned_l] = planned_v\n",
    "            details['Date_venue'] = date_ven_lst\n",
    "            details['Auction_name'] = auction_name\n",
    "            details['Vehicle_Location'] = veh_location\n",
    "            details['Lot_number'] = lot_num\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"No reg and name found\")\n",
    "\n",
    "    # =====================================================================================  \n",
    "        # get the car images\n",
    "        try:\n",
    "            images = []\n",
    "            img_cont_class = \"Thumbnails__ImagesContainer-sc-tbif6y-0\"\n",
    "            img_cont = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, img_cont_class)))\n",
    "\n",
    "            # get the images now\n",
    "            imgs = img_cont.find_elements(By.TAG_NAME, \"img\")\n",
    "            for img in imgs:\n",
    "                images.append(img.get_attribute(\"src\"))\n",
    "            imgs_str = \", \".join(images)\n",
    "            details['Images'] = imgs_str\n",
    "        except Exception as e:\n",
    "            print(\"No images found\")\n",
    "\n",
    "    # =====================================================================================  \n",
    "        # get the SPECIFICATION\n",
    "        try:\n",
    "            main_pg = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".cDPOzx\")))\n",
    "            \n",
    "            labels_css = \".cCprlp\"\n",
    "            # divs = WebDriverWait(driver, 60).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, divs_css)))\n",
    "            labels = main_pg.find_elements(By.CSS_SELECTOR, labels_css)\n",
    "            labels_list = []\n",
    "\n",
    "            # get all labels\n",
    "            for label in labels:\n",
    "                labels_list.append(label.text)\n",
    "\n",
    "            values_css = \".khatBv\"\n",
    "            values = main_pg.find_elements(By.CSS_SELECTOR, values_css)\n",
    "            values_list = []\n",
    "\n",
    "            #get all the values\n",
    "            for value in values:\n",
    "                values_list.append(value.text)\n",
    "            \n",
    "            # append the labels and values in the dictionary\n",
    "            temp_dict=dict(zip(labels_list, values_list))\n",
    "            # update the existance dictionary\n",
    "            details.update(temp_dict)\n",
    "        except Exception as e:\n",
    "            print(\"No divs\")\n",
    "\n",
    "    # ===================================================================================== \n",
    "        # get the decleration\n",
    "        try:\n",
    "            dec_main = driver.find_element(By.CSS_SELECTOR, \"div.dRCqKq\")\n",
    "            dec_label = dec_main.find_element(By.CSS_SELECTOR, \"h5.hRqhbX\").text\n",
    "            dec_values = dec_main.find_elements(By.CSS_SELECTOR, \"p.gHHIJJ\")\n",
    "            if len(dec_values) >= 1:\n",
    "                dec_list = []\n",
    "                for dec_v in dec_values:\n",
    "                    dec_list.append(dec_v.text)\n",
    "                details[dec_label] = dec_list\n",
    "            else:\n",
    "                details[dec_label] = \"No decelartions found\"\n",
    "\n",
    "        except Exception as e:\n",
    "            details[\"Declerations\"] = \"No decelartions found\"\n",
    "    # =====================================================================================\n",
    "        # get the equipment and additional information\n",
    "        try:\n",
    "            # Locate all sections with the same class\n",
    "            main_sections = driver.find_elements(By.CSS_SELECTOR, \"div.CATTm\")\n",
    "\n",
    "            # Loop through all sections to identify and extract the correct data\n",
    "            for section in main_sections:\n",
    "                try:\n",
    "                    # Get the label of the section, regardless of its CSS class\n",
    "                    section_label = section.find_element(By.CSS_SELECTOR, \"h5\").text.lower()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Section label not found:\", e)\n",
    "                    continue  # Skip this section if label is missing\n",
    "\n",
    "                # Identify the equipment section\n",
    "                if \"equipment\" in section_label:\n",
    "                    try:\n",
    "                        eq_header = section.find_element(By.CSS_SELECTOR, \"div.eVCHPp\")\n",
    "                        eq_values = eq_header.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "                        if len(eq_values) > 0:\n",
    "                            eq_list = [eq_value.text for eq_value in eq_values]\n",
    "                            details[\"Equipment\"] = eq_list\n",
    "                        else:\n",
    "                            details[\"Equipment\"] = \"No equipment found\"\n",
    "                    except Exception as e:\n",
    "                        print(\"Error extracting equipment details:\", e)               \n",
    "\n",
    "                # Identify the additional information section\n",
    "                elif \"additional information\" in section_label:\n",
    "                    try:\n",
    "                        add_header = section.find_element(By.CSS_SELECTOR, \"div.eVCHPp\")\n",
    "                        add_values = add_header.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "                        if len(add_values) > 0:\n",
    "                            add_list = [add_value.text for add_value in add_values]\n",
    "                            details[\"Additional Information\"] = add_list\n",
    "                        else:\n",
    "                            details[\"Additional Information\"] = \"No additional information found\"\n",
    "                    except Exception as e:\n",
    "                        print(\"Error extracting additional information details:\", e)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "\n",
    "    # =====================================================================================\n",
    "        # get the conditon report\n",
    "        try:\n",
    "            cond_sec = driver.find_element(By.CSS_SELECTOR, \"div.kjlvHR\")\n",
    "            grade_label = cond_sec.find_element(By.CSS_SELECTOR, \"div.YpBMK\").text\n",
    "            grade_val = cond_sec.find_element(By.CSS_SELECTOR, \"div.frxkxC\").text\n",
    "            details[grade_label] = grade_val\n",
    "\n",
    "            # get the text\n",
    "            cond_text = cond_sec.find_element(By.CSS_SELECTOR, \"p.fBEwrJ\")\n",
    "            details['Condition_Report_text'] = cond_text.text\n",
    "\n",
    "            # get the link\n",
    "            cond_rep = cond_sec.find_element(By.XPATH, \"//a[text()='View Condition Report']\")\n",
    "            report = cond_rep.get_attribute('href')\n",
    "            details[\"Condition_report\"] =  report\n",
    "\n",
    "        except Exception as e:\n",
    "            details[\"Condition report\"] = \"No condition report found\"\n",
    "\n",
    "    # ===================================================================================== \n",
    "        # get the conditon report\n",
    "        try:\n",
    "            assured_sec = driver.find_element(By.CSS_SELECTOR, \"div.eqmWdV\")\n",
    "            # get the text\n",
    "            assured_text = assured_sec.find_element(By.CSS_SELECTOR, \"p.iYoHiO\")\n",
    "            details['Condition_Report_text'] = assured_text.text\n",
    "\n",
    "            # get the link\n",
    "            assured_rep = assured_sec.find_element(By.XPATH, \"//a[text()='View Assured Report']\")\n",
    "            report = assured_rep.get_attribute('href')\n",
    "            details[\"Assured_report\"] =  report\n",
    "\n",
    "        except Exception as e:\n",
    "            details[\"Condition report\"] = \"No condition report found\"\n",
    "\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            # Locate the main base div with class XEQhH\n",
    "            base_div = driver.find_element(By.CSS_SELECTOR, \"div.XEQhH\")\n",
    "            # get the CAP HPI\n",
    "            try:\n",
    "                cap_hpi_sec = base_div.find_element(By.CSS_SELECTOR, \"div.iUYhNI\")\n",
    "                cap_hpi = cap_hpi_sec.find_element(By.CSS_SELECTOR, \"h6.fkQpmB\").text\n",
    "\n",
    "                # get all the column names \n",
    "                cols = cap_hpi_sec.find_elements(By.CSS_SELECTOR, \".hcEGbM\")\n",
    "                cols_list = []\n",
    "                for col in cols:\n",
    "                    cols_list.append(col.text)\n",
    "\n",
    "                # get all values\n",
    "                vals = cap_hpi_sec.find_elements(By.CSS_SELECTOR, \".bwOa-dQ\")\n",
    "                val_list=[]\n",
    "                for val in vals:\n",
    "                    val_list.append(val.text)\n",
    "                temp_dict_2 = dict(zip(cols_list, val_list))\n",
    "                details[cap_hpi] = temp_dict_2\n",
    "            except Exception as e:\n",
    "                details['CAP HPI'] = \"no details found for cap hpi pricing\"\n",
    "            # ===================================================================\n",
    "            try:\n",
    "                # get the Glass's\n",
    "                glass_sec = base_div.find_element(By.CSS_SELECTOR, \"div.ShvzP\")\n",
    "                glass = glass_sec.find_element(By.CSS_SELECTOR, \"h6.kKa-DOe\").text\n",
    "\n",
    "                # get all the column names \n",
    "                g_cols = glass_sec.find_elements(By.CSS_SELECTOR, \".hcEGbM\")\n",
    "                g_cols_list = []\n",
    "                for g_col in g_cols:\n",
    "                    g_cols_list.append(g_col.text)\n",
    "\n",
    "                # get all values\n",
    "                g_vals = glass_sec.find_elements(By.CSS_SELECTOR, \".bwOa-dQ\")\n",
    "                g_val_list=[]\n",
    "                for g_val in g_vals:\n",
    "                    g_val_list.append(g_val.text)\n",
    "                g_temp_dict_2 = dict(zip(g_cols_list, g_val_list))\n",
    "                details[glass] = g_temp_dict_2\n",
    "            except Exception as e:\n",
    "                details['Glass'] = \"no details found for glass pricing\"\n",
    "            # ===================================================================================\n",
    "            try:\n",
    "                get_text = base_div.find_element(By.CSS_SELECTOR, \"h6.iGcAgx\")\n",
    "                details['Pricing_text'] = get_text.text\n",
    "            except Exception as e:\n",
    "                details['Pricing_text'] = \"no texts of pricing found\"\n",
    "            # =====================================================================================\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error occurred:\", e)\n",
    "    # =====================================================================================\n",
    "\n",
    "        results.append(details)\n",
    "        next_button = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"next-lot-navigation-button\"]')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "        next_button.click()\n",
    "        i+=1\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(\"bca.csv\")\n",
    "    driver.quit()\n",
    "path = \"https://www.bca.co.uk/lot/RE67%20HXA?q=&bq=VehicleType%3ACars&searchVersion=new\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Reg_No', 'Name', 'Mileage', 'Featured in', 'Confirmed',\n",
       "       'Date_venue', 'Auction_name', 'Vehicle_Location', 'Lot_number',\n",
       "       'Images', 'Vehicle Type', 'Colour', 'Fuel', 'Transmission',\n",
       "       'No. of doors', 'CO2 Emissions', 'NOX Emissions', 'No. of keys',\n",
       "       'Log book', 'No. of owners', 'Date of registration', 'VAT Type',\n",
       "       'Service History', 'Number of services', 'Last Service',\n",
       "       'Last service mileage', 'DVSA mileage', 'Additional service notes',\n",
       "       'MOT Expiry', 'Declarations', 'Equipment', 'Grade',\n",
       "       'Condition_Report_text', 'Condition_report', 'Assured_report',\n",
       "       'CAP HPI*', 'Glass's', 'Pricing_text', 'Condition report',\n",
       "       'Declerations', 'Planned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bca.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now download the images from the links in the above df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www1.bcaimage.com/Document?DocType=VehicleImage&Reg=RE67HXA-GB&grp=public&obl=hht3,Manheim&minwidth=600&width=600&default=5, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395278, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395281, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395285, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395290, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395296, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395303, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395315, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395318, https://www1.bcaimage.com/Document?DocType=VehicleImage&width=600&docId=561395323, https://www1.bcaimage.com/Document?DocType=VehicleImage&docId=561288154, https://www1.bcaimage.com/Document?DocType=VehicleImage&docId=561288155'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate the reg_no and images columns from the above dataframe\n",
    "reg_img = df[['Reg_No', 'Images']]\n",
    "reg_img['Images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_1_image_1.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_2_image_2.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_3_image_3.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_4_image_4.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_5_image_5.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_6_image_6.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_7_image_7.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_8_image_8.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_9_image_9.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_10_image_10.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_11_image_11.jpg\n",
      "Downloaded: Images\\RE67 HXA\\RE67 HXA_12_image_12.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_1_image_1.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_2_image_2.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_3_image_3.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_4_image_4.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_5_image_5.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_6_image_6.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_7_image_7.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_8_image_8.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_9_image_9.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_10_image_10.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_11_image_11.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_12_image_12.jpg\n",
      "Downloaded: Images\\LK20 UOF\\LK20 UOF_13_image_13.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_1_image_1.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_2_image_2.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_3_image_3.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_4_image_4.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_5_image_5.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_6_image_6.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_7_image_7.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_8_image_8.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_9_image_9.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_10_image_10.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_11_image_11.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_12_image_12.jpg\n",
      "Downloaded: Images\\YT17 FHJ\\YT17 FHJ_13_image_13.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_1_image_1.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_2_image_2.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_3_image_3.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_4_image_4.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_5_image_5.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_6_image_6.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_7_image_7.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_8_image_8.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_9_image_9.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_10_image_10.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_11_image_11.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_12_image_12.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_13_image_13.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_14_image_14.jpg\n",
      "Downloaded: Images\\GY18 SNX\\GY18 SNX_15_image_15.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_1_image_1.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_2_image_2.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_3_image_3.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_4_image_4.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_5_image_5.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_6_image_6.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_7_image_7.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_8_image_8.jpg\n",
      "Downloaded: Images\\YV19 ULD\\YV19 ULD_9_image_9.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "# urlparse: Parses a URL into components (scheme, netloc, path, etc.), making it easy to validate or extract information from the URL.\n",
    "# urljoin: Joins a base URL with a relative path to form a complete URL.\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"): # here the main folder is Images\n",
    "    \n",
    "    # Create the main folder if it doesn't exist\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    # loop around every row to get the info\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg_No\"] # separate reg nums\n",
    "        image_urls = row[\"Images\"].split(\", \")  # Split image URLs by comma if multiple\n",
    "        \n",
    "        # Create a subfolder for the car registration number\n",
    "        reg_folder = os.path.join(main_folder, reg_no) # combine the main folder name and the reg num like Image/reg_no\n",
    "        os.makedirs(reg_folder, exist_ok=True) # create the subfolder of the type mention in the above line of code\n",
    "        \n",
    "        # loop around the urls of the current row and also save index for further use\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip()  # Remove extra spaces\n",
    "            if not url.startswith((\"http://\", \"https://\")): # check if the url does not start with the values in the bracket\n",
    "                url = urljoin(\"https://\", url) # sets the urls starting from https://.....\n",
    "            \n",
    "            # parse the url\n",
    "            parsed_url = urlparse(url)\n",
    "\n",
    "            # check if the parsed url is valid\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\") # incase some urls are incorret and are not loaded for downloading \n",
    "                continue\n",
    "            \n",
    "            # try downloading images\n",
    "            try:\n",
    "                # Download the image\n",
    "                response = requests.get(url, stream=True) # send the url for downloading\n",
    "                response.raise_for_status() # Raises an exception if the HTTP request fails (e.g., 404 or 500).\n",
    "                \n",
    "                # Extracts the file name from the URL path (e.g., image.jpg from http://example.com/image.jpg).\n",
    "                # If no file name is found, assigns a default name based on the index.\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                # Extracts the file extension (e.g., jpg). from the last index value\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "                # Ensure the file has a valid extension\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\" # set the extension if the extension is not in the above list\n",
    "                \n",
    "                # Construct the full path for the file\n",
    "                full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "                \n",
    "                # Save the image\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024): # Reads the response in 1KB chunks to save memory.\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "# Call the function\n",
    "download_images(reg_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
