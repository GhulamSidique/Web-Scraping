{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 3 of cars found\n",
      "No next button found hence no more cars are available\n"
     ]
    }
   ],
   "source": [
    "def scrape(path):\n",
    "\n",
    "    # to avoid windows to close again and again we make use of headless\n",
    "    options = ChromeOptions()\n",
    "    options.headless=True\n",
    "    # make use of chrome for scraping\n",
    "    service = Service(r\"D:\\GS_WebScarping\\project_1\\chromedriver.exe\")\n",
    "    # create a driver using chrome\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    # run the driver through url\n",
    "    driver.get(path)\n",
    "\n",
    "# =====================================================================================\n",
    "    # get the login icon and click on it \n",
    "# =====================================================================================\n",
    "    try:\n",
    "        # get the login button\n",
    "        login = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".acc_button_alt\")))\n",
    "        if login:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", login)\n",
    "            login.click()\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"No login tab found and the error is {e}\")\n",
    "\n",
    "# =====================================================================================\n",
    "    # complete logiin info\n",
    "# =====================================================================================\n",
    "\n",
    "    # get the username tab\n",
    "    try:\n",
    "        provided_u_name = \"sultanmirza0501@gmail.com\"\n",
    "        user_name = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.ID,\"Username\")))   \n",
    "        user_name.send_keys(provided_u_name)\n",
    "    except Exception as e:\n",
    "        print(f\"No username tab found and the error is {e}\")\n",
    "\n",
    "    # get password tab\n",
    "    try:\n",
    "        provided_pass = \"Muhssan7865\"\n",
    "        password = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.ID, \"Password\")))\n",
    "        password.send_keys(provided_pass)\n",
    "    except Exception as e:\n",
    "        print(f\"No password tab found and the error is {e}\")\n",
    "\n",
    "    # get the login tab to enter the username and password\n",
    "    try:\n",
    "        # login_tab_css = \"submit\"\n",
    "        login_tab = driver.find_element(By.XPATH,  \"//input[@type='submit' and @value='Login']\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", login_tab)\n",
    "        login_tab.click()\n",
    "        for i in range(2):\n",
    "            driver.back()\n",
    "    except Exception as e:\n",
    "        print(f\"No login tab found and the error is {e}\")\n",
    "\n",
    "# =====================================================================================\n",
    "    # handle cookies\n",
    "# =====================================================================================\n",
    "    try:\n",
    "        cookie = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.ID, \"onetrust-reject-all-handler\")))\n",
    "        if cookie:\n",
    "            cookie.click()\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(\"No cookies tab found and the error is {e}\")\n",
    "\n",
    "# =====================================================================================\n",
    "    # get only number\n",
    "# =====================================================================================\n",
    "    try:\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "        num_cars = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".d-md-flex span\"))).text\n",
    "        cars_list = num_cars.split(\" \")\n",
    "        cars =  int(cars_list[1]) # get only number\n",
    "        print(f\"Number {cars} of cars found\")\n",
    "    except Exception as e:\n",
    "        print(f\"No number of cars found and error is {e}\")\n",
    "\n",
    "\n",
    "# # get the first car page link\n",
    "# try:\n",
    "#     first_car_page = driver.find_element(By.CSS_SELECTOR, \"a.vehicle-card-link\")\n",
    "#     if first_car_page:\n",
    "#         first_car_page_link = first_car_page.get_attribute(\"href\")\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "# =====================================================================================\n",
    "    # get the car link and click on it to proceed to main car information page\n",
    "# =====================================================================================\n",
    "    try:\n",
    "        link = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CSS_SELECTOR, \".vehicle-card-header\")))\n",
    "        if link:\n",
    "        # now we have to click on the link to get the main car info page\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", link)\n",
    "            link.click()\n",
    "        else:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(f\"No link found and the error is {e}\")\n",
    "# =====================================================================================  \n",
    "\n",
    "\n",
    "# =====================================================================================\n",
    "    # loop around the number of images present\n",
    "# =====================================================================================\n",
    "    results = [] # to append the dict values later on\n",
    "    for i in range(cars):# we will make use of number of pages here\n",
    "\n",
    "    # ===================================================================================== \n",
    "        # get the name of the car\n",
    "    # =====================================================================================\n",
    "        details = {}\n",
    "        \n",
    "        try:\n",
    "            # car_name_css = \".row h2\"\n",
    "            car_name = driver.find_element(By.CSS_SELECTOR, \".row h2\")\n",
    "            if car_name:\n",
    "                details[\"Car_name\"] = car_name.text\n",
    "            else:\n",
    "                details[\"Car_name\"] = \"No car name for this car found\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No car name found and the error is {e}\")\n",
    "        \n",
    "    # ===================================================================================== \n",
    "        # extract the car images \n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            images=[]\n",
    "            # import time\n",
    "            # time.sleep(2)\n",
    "            imgs = driver.find_elements(By.CSS_SELECTOR, \"li.slide img\")\n",
    "\n",
    "            for img in imgs:\n",
    "                images.append(img.get_attribute(\"src\"))\n",
    "\n",
    "            imgs_str = \", \".join(images)\n",
    "            details['Images'] = imgs_str\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No images found and the error is {e}\")\n",
    "\n",
    "    # =====================================================================================         \n",
    "        # get the lot time, date, auction and center fields\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            # container_selector = \".ab-catelogue-vehicle-details-auction-details\"\n",
    "            container = driver.find_element(By.CSS_SELECTOR, \".ab-catelogue-vehicle-details-auction-details\")\n",
    "\n",
    "            # Extract Auction Availability\n",
    "            auction_availability = container.find_element(By.TAG_NAME, \"p\").text\n",
    "            if auction_availability:\n",
    "                details[\"Auction_Availability\"] = auction_availability\n",
    "            else:\n",
    "                details[\"Auction_Availability\"] = \"No auction availability found\"\n",
    "\n",
    "            # Extract Lot, Time, and Date\n",
    "            lot_time_date = container.find_elements(By.CSS_SELECTOR, \"h3 span\")\n",
    "            if lot_time_date:\n",
    "                lot = lot_time_date[0].text.replace(\"Lot: \", \"\")  # Removing 'Lot: ' prefix\n",
    "                time = lot_time_date[1].text\n",
    "                date = lot_time_date[2].text\n",
    "                details['Lot'] = lot\n",
    "                details['Time']= time\n",
    "                details['Date'] = date\n",
    "            else:\n",
    "                details['Lot'] = \"No lot number found\"\n",
    "                details['Time']= \"No time found\"\n",
    "                details['Date'] = \"No date found\"\n",
    "\n",
    "\n",
    "            # Extract Center\n",
    "            center = container.find_element(By.CSS_SELECTOR, \"h3:nth-of-type(2) span\").text\n",
    "            if center:\n",
    "                details[\"Center\"] = center\n",
    "            else:\n",
    "                details['Center'] = \"No center found\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Nothing found and the error is {e}\")\n",
    "        \n",
    "    # =====================================================================================\n",
    "        # get the vehicle details\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            # click on vehicle tab\n",
    "            veh_tab = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"VEHICLE\")))\n",
    "            if veh_tab:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", veh_tab)\n",
    "                veh_tab.click()\n",
    "\n",
    "                base_card = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"tab-content\")))\n",
    "                \n",
    "                # Locate the table inside the card\n",
    "                table_selector = \".tab-pane.active .table tbody tr\"\n",
    "                rows = base_card.find_elements(By.CSS_SELECTOR, table_selector)\n",
    "\n",
    "                if rows:\n",
    "                    for row in rows:\n",
    "                        # Find all <td> elements in the row\n",
    "                        tds = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                        if len(tds) == 2:  # Ensure there are exactly two <td> elements\n",
    "                            label = tds[0].text.strip()\n",
    "                            value = tds[1].text.strip()\n",
    "                            details[label] = value\n",
    "                else:\n",
    "                    print(\"No rows found\")\n",
    "            else:\n",
    "                print(\"No vehicle tab found\")\n",
    "        except Exception as e:\n",
    "            print(f\"Novehcile info found and the error is {e}\")\n",
    "\n",
    "    # =====================================================================================\n",
    "        # get the valuations\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "\n",
    "            valuation = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"VALUATIONS\")))\n",
    "            if valuation:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", valuation)\n",
    "                valuation.click()\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "\n",
    "                val_card = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"tab-content\")))\n",
    "                if val_card:\n",
    "                    val_rows = val_card.find_elements(By.CSS_SELECTOR, \".tab-pane.active .table tbody tr\")\n",
    "                    if val_rows:         \n",
    "                        for val_row in val_rows:\n",
    "                            # Find all <td> elements in the row\n",
    "                            val_tds = val_row.find_elements(By.TAG_NAME, \"td\")\n",
    "                            if len(tds) == 2:  # Ensure there are exactly two <td> elements\n",
    "                                val_label = val_tds[0].text.strip()\n",
    "                                val_value = val_tds[1].text.strip()\n",
    "                                details[val_label] = val_value\n",
    "                    else:\n",
    "                        print(\"No value rows found inside valaution card\")\n",
    "                else:\n",
    "                    print(\"No valuation card found\")\n",
    "            else:\n",
    "                print(\"No valaution button found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"No valaution button found and the error is {e}\")\n",
    "        \n",
    "    # =====================================================================================\n",
    "        # get the INSPECTION REPORT\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "\n",
    "            inspection = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, \"INSPECTION REPORT\")))\n",
    "            if inspection:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", inspection)\n",
    "                inspection.click()\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "            \n",
    "                # Now extract the fields inside the inspection report\n",
    "                inspec_card = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"tab-content\")))\n",
    "            \n",
    "            # =====================================================================================\n",
    "                # get the icon image link at the side of the name of the car\n",
    "            # =====================================================================================\n",
    "                try:\n",
    "                    # inspec_rep_css = WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.XPATH, \"//a[text()='Download the inspection report']\")))\n",
    "                    inspec_rep_css = driver.find_element(By.XPATH, \"//a[text()='Download the inspection report']\")\n",
    "                    if inspec_rep_css:\n",
    "                        inspec_link = inspec_rep_css.get_attribute('href')\n",
    "                        details[\"Inspection_report\"] =  inspec_link\n",
    "                    else:\n",
    "                        details[\"Inspection_report\"] = \"No inspection report found\"\n",
    "                except Exception as e:\n",
    "                    print(\"No car inspection report tab found\")\n",
    "\n",
    "            # ===================================================================================== \n",
    "                # get the icon image link at the side of the name of the car\n",
    "            # =====================================================================================\n",
    "                try:\n",
    "                    icon_image_link = inspec_card.find_element(By.XPATH, \"//a[contains(@href, '.pdf')]\")\n",
    "\n",
    "                    if icon_image_link:\n",
    "                        icon_href = icon_image_link.get_attribute(\"href\")\n",
    "                        details['Icon_image_link'] = icon_href\n",
    "                    else:\n",
    "                        details['Icon_image_link'] = \"N/A\"\n",
    "                except Exception as e:\n",
    "                    print(f\"No icon image tab found\")\n",
    "\n",
    "                \n",
    "                # Locate the table inside the card (looking specifically for the 'Tyres' table)\n",
    "                tyres_table = inspec_card.find_element(By.XPATH, \"//h3[contains(text(), 'Tyres')]//following::table[1]\")\n",
    "                tyres_rows = tyres_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "                # Extract other fields as before (standard inspection fields)\n",
    "                inspec_table_selector = \".tab-pane.active .table tbody tr\"\n",
    "                inspec_rows = inspec_card.find_elements(By.CSS_SELECTOR, inspec_table_selector)\n",
    "                \n",
    "                for inspec_row in inspec_rows:\n",
    "                    inspec_tds = inspec_row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    \n",
    "                    # If there are exactly two <td> elements, process the label-value pair\n",
    "                    if len(inspec_tds) == 2:\n",
    "                        inspec_label = inspec_tds[0].text.strip()\n",
    "                        inspec_value = inspec_tds[1].text.strip()\n",
    "                        details[inspec_label] = inspec_value\n",
    "                # Lists to store values for Location, Manufacturers, and Depth\n",
    "                location_values = []\n",
    "                manufacturers_values = []\n",
    "                depth_values = []\n",
    "\n",
    "                # Process each row inside the Tyres table\n",
    "                for row in tyres_rows:\n",
    "                    tds = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    if len(tds) == 3:  # We expect three columns for Location, Manufacturers, Depth\n",
    "                        location_values.append(tds[0].text.strip())\n",
    "                        manufacturers_values.append(tds[1].text.strip())\n",
    "                        depth_values.append(tds[2].text.strip())\n",
    "\n",
    "                        details['Location'] = \", \".join(location_values)\n",
    "                        details['Manufacturers'] = \", \".join(manufacturers_values)\n",
    "                        details['Depth'] = \", \".join(depth_values)\n",
    "            else:\n",
    "                print(\"No inspection button found\")\n",
    "        except Exception as e:\n",
    "            print(f\"No inspection button found and error is {e}\")\n",
    "        \n",
    "    # =====================================================================================\n",
    "        # get the SPECIFICATION\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            # get the button and then click on it\n",
    "            spec_text = \"SPECIFICATION\"\n",
    "            specs = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, spec_text)))\n",
    "            if specs:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", specs)\n",
    "                specs.click()\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "\n",
    "                # now extract the fields inside valautions\n",
    "                spec_card_class = \"tab-content\"\n",
    "                spec_card = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, spec_card_class)))\n",
    "                \n",
    "                # Locate the table inside the card\n",
    "                spec_table_selector = \".tab-pane.active .table tbody tr\"\n",
    "                spec_rows = spec_card.find_elements(By.CSS_SELECTOR, spec_table_selector)\n",
    "                \n",
    "                td_texts = [td.text for td in spec_rows if td.text.strip() != \"\"]\n",
    "                details['Specifications']=td_texts\n",
    "            else:\n",
    "                print(\"No specs button found\")\n",
    "        except Exception as e:\n",
    "            print(f\"No specification button found and error is {e}\")\n",
    "\n",
    "\n",
    "    # =====================================================================================\n",
    "        # Extract the image source URL\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            damage_text = \"DAMAGE REPORT\"\n",
    "            damages = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.LINK_TEXT, damage_text)))\n",
    "            if damages:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", damages)\n",
    "                damages.click()\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Initialize list for storing damaged image URLs and key-value pairs\n",
    "                damaged_list = []\n",
    "                details_list = []\n",
    "\n",
    "                # Extract all image elements with the class 'magnifier-image'\n",
    "                img_elements = driver.find_elements(By.CLASS_NAME, 'magnifier-image')\n",
    "                for img in img_elements:\n",
    "                    damaged_list.append(img.get_attribute(\"src\"))\n",
    "                \n",
    "                # Assuming there's only one image to focus on for now\n",
    "                damaged_urls = \", \".join(damaged_list)\n",
    "                details[\"Damaged_image\"] = damaged_urls\n",
    "\n",
    "                # Extract key-value pairs from the damage table\n",
    "                damage_rows = driver.find_elements(By.CSS_SELECTOR, '.tab-pane.active .table tbody tr')\n",
    "\n",
    "                for damage_row in damage_rows:\n",
    "                    # Get the first and second <td> elements\n",
    "                    columns = damage_row.find_elements(By.TAG_NAME, 'td')\n",
    "                    if len(columns) >= 2:  # Ensure there are exactly two columns\n",
    "                        column_name = columns[0].text.strip()\n",
    "                        column_value = columns[1].text.strip()\n",
    "                        details_list.append({column_name: column_value})\n",
    "\n",
    "                details[\"Damage_details\"] = details_list  # Add to the 'details' dictionary\n",
    "            else:\n",
    "                print(\"No damage button found\")\n",
    "        except Exception as e:\n",
    "            print(f\"No damage button found and error is {e}\")\n",
    "    # =====================================================================================\n",
    "    # =====================================================================================\n",
    "\n",
    "        # append the in the results \n",
    "        results.append(details)\n",
    "\n",
    "    # =====================================================================================\n",
    "        # get the next button\n",
    "    # =====================================================================================\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"a.vehicle-details-btn-next\")\n",
    "            if next_button:\n",
    "                # get the link\n",
    "                car_page_link = next_button.get_attribute(\"href\")\n",
    "                details['Car_link']= car_page_link\n",
    "                next_button.click()\n",
    "            else:\n",
    "                details['Car_link'] = \"N/A\"\n",
    "                \n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(\"No next button found hence no more cars are available\")\n",
    "\n",
    "# =====================================================================================\n",
    "    # create a dataframe \n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(\"data2.csv\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# call the function\n",
    "path = \"https://catalogue.astonbarclay.net/vehicle-search/?search=true&sites=1&sales=14734\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here the car_link column contains the list of the car page links, the first link belongs to the second car and rest follow this series, the last row will have the second last link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 8 of cars found\n",
      "No car inspection report tab found\n",
      "No icon image tab found\n",
      "No next button found hence no more cars are available\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://catalogue.astonbarclay.net/details/catalogue/108063371'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data2.csv\")\n",
    "df['Car_link'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to download the original and damage images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_1_659a7495-cfdc-447b-aec8-3bd945ccdab7_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_2_0aba4f81-e5c5-4c8e-b1f5-b4f8489c5d22_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_3_1d0b624a-a0c6-4fc6-9a23-852eeed01f85_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_4_44d6b676-5ab4-4bcc-b208-8dadf0ab037c_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_5_8a048c20-24e7-4474-8f75-815bbe2e20c5_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_6_3538558f-aaaf-4263-b641-2bdf4c4cc06f_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_7_9e4cdb45-3690-44fa-aec8-7f303962b57d_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_8_bdfa6045-1857-46f6-a77e-aab3c12ddd6c_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Original_images\\BL17CMU_9_281d31e3-03dd-4a56-b24f-8c171b9c7986_removebg.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_1_7cdcc779-a594-41c8-a327-5153df89a39f_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_2_efac590e-8a8a-4b25-bea2-1df7f064edd3_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_3_bee8b50e-6fb5-493e-ac6f-0c2436e80949_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_4_718c3f7a-7269-4ca2-a879-a8233e8a4f53_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_5_5561a3b9-1d6f-4ab7-8838-bd7afc25f8f4_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_6_no-image.png\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_7_no-image.png\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_8_321d5098-9b56-41d8-9e49-58ca5ec6e535_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_9_d179f15b-e89f-49e8-9310-331865924313_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_10_7a048b8d-27d9-474a-b248-8b01bca9368b_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_11_362956be-cb6b-4770-92d6-4012edbbe81c_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_12_089d9051-05b2-4c71-bec3-8e0b64849c9f_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_13_9215021c-2499-4b03-8fde-e4d06e62adba_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_14_b5e6e3b4-4a36-45dd-b354-3469744c222b_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_15_3d76dcdd-e7dd-4643-850d-57a80e49ab3c_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_16_8116764d-3239-4cbf-a438-4fcbdd62b45c_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_17_74990137-8d8c-4cd4-b6bd-38678e07aed2_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_18_27a23f74-122d-4c50-acc5-ca7d27bfc2ab_1.jpg\n",
      "Downloaded: Images\\BL17CMU\\Damaged_images\\BL17CMU_19_cb555f33-7853-4dcf-9c19-e3ce1e620868_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_1_5f936e8a-eae2-45fd-a6eb-c910ffb23cfa_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_2_e43d037f-0e99-41d9-8085-c612e5c52fd6_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_3_a36446e2-590e-44c2-94d4-08c228f8d538_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_4_13ffa9a2-648f-4648-b8b6-ecccd3f8a086_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_5_d1d50620-3259-40bb-bfb4-1f522ec22953_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_6_6a0845ec-7081-414a-a156-8921a34fd5c2_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_7_d8cbb02e-86fc-4071-bc3d-2dbcf7b7b42d_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_8_6e03311e-eb59-468b-88bd-f5b698563ff9_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Original_images\\WD68JVP_9_116c0135-fde6-4720-aedb-d90420bee64c_removebg.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_1_no-image.png\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_2_5ca853b9-025a-4b3f-8c3e-772548b1a0b0_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_3_no-image.png\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_4_fd5e0373-a419-4f94-898d-a788f311f207_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_5_no-image.png\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_6_6043316a-5746-4f40-a912-e49e84c2a39f_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_7_5ef1b49e-9697-4358-ae38-85d08d2f1fb2_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_8_410dd575-c6fd-4a6b-b3e8-8e1cc89a9d54_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_9_no-image.png\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_10_d17cb425-8b75-4830-8526-18276a2ce2d6_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_11_51bb3125-64fd-4de5-8fd9-7034bf9ad74f_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_12_cbc7f5ba-1000-4b54-b244-fc6d8e0e8416_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_13_27ce727d-ef48-4d4c-9751-5a20c94a742a_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_14_515ca4ff-5be2-4b6b-a79c-ba55a931be16_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_15_cd99aef1-f401-4a05-985e-1be39cc9826b_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_16_7e3cfd96-8acd-4e59-b1cf-0706e41903bf_1.jpg\n",
      "Downloaded: Images\\WD68JVP\\Damaged_images\\WD68JVP_17_38aafcf0-6df6-4eae-8096-e9baaedc195b_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_1_75846576-e9a1-43e8-b0d3-910eb2c3c9a9_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_2_98e3c42f-b5bc-41ba-8bc9-1a0df08a79fa_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_3_fd24c273-8a58-4ba1-b14f-c42ef126a204_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_4_c196682d-21af-4b6f-9f43-375598ec4c05_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_5_4d96f634-36e4-4241-b705-2e7d65b4e368_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_6_623ffc93-9413-4b2c-8c5d-b499aa710b32_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_7_5bfd8023-1f87-4f25-998f-5cd040da7571_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Original_images\\BV67WPP_8_cf092ef8-e4cc-49aa-8731-cb35bc0cfb5f_removebg.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_1_511836e5-553d-41fb-bb7b-99095fb0a1c3_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_2_dad4a6e2-90cc-4430-a834-387236773ca4_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_3_31ded6ec-d7e2-4282-bff0-5bcb943fd445_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_4_435bba5f-09a5-4e12-8e47-a681645062be_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_5_fb59d794-82d5-46d9-bc88-cf19af3f8b37_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_6_640fa3ef-c46c-4775-bbdc-c6876fe6c643_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_7_a109b3f8-8dd2-4943-bdc2-b162c328e0ea_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_8_f02b8fe9-1e09-48e0-99e9-20068901e02f_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_9_c70aec96-2ff2-49cf-8778-bbfcf9b02e30_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_10_d183aa0a-aebf-449e-b3a3-cc7c494de1d6_1.jpg\n",
      "Downloaded: Images\\BV67WPP\\Damaged_images\\BV67WPP_11_no-image.png\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_1_44110ca2-06ef-43e2-b947-ce3ba0aded4f_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_2_19143f0a-8f20-4975-80ea-ab1af36a5b87_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_3_01a2d48b-46c7-4b67-8dc5-98af7b06afe9_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_4_5f4f4d3f-5c58-42ea-a8b8-c1f5fbcdeb57_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_5_7921487c-2a06-44a2-8915-492c036f8d4d_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_6_9881b7d0-1ecd-47a3-ab0d-69e44542af8e_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_7_e67fe604-4570-417f-8466-a1600da8aa93_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Original_images\\DL19YHX_8_c3d2bbf4-d847-4fdb-881a-0a38ebf725a8_removebg.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_1_ffbb0b90-011f-4e12-9a8a-5d22dc52fa35_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_2_8182610b-cdc8-430a-802b-00ffc6cfc9ef_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_3_45afd882-2d6c-4a41-8168-95803391fefb_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_4_ad0a83fd-9197-424e-8618-87b6965dfd7d_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_5_8132091c-2ce1-49a5-8892-1d71060011bb_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_6_04ff8292-0faf-483e-8bd0-d962239b0182_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_7_be6ae933-7111-4c57-8b17-e490773fd794_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_8_6b24c58a-6a03-4e44-8401-1ce96a59bf5e_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_9_d7863389-eb35-4190-82b6-7638b2dcec97_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_10_f0b77e97-142a-4e28-930c-26812e41c1f5_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_11_fca9e845-9318-4762-94cb-739d142c5ca4_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_12_174f18da-c54a-4ee7-9d29-7f0a145a553e_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_13_200402b6-7d88-42fe-8d35-a8f93ba9f2f1_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_14_dda92ab5-cbd8-4652-b065-cd13ab73df71_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_15_3557d8de-d212-4209-983b-cff2ee212447_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_16_721dbf43-0017-4a44-af65-11b3409fa342_1.jpg\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_17_no-image.png\n",
      "Downloaded: Images\\DL19YHX\\Damaged_images\\DL19YHX_18_d957e707-d005-4325-a925-806d4f0971a4_1.jpg\n"
     ]
    }
   ],
   "source": [
    "reg_img = df[['REG', \"Images\", \"Damaged_image\"]]\n",
    "\n",
    "from urllib.parse import urlparse, urljoin\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    # Create the main folder if it doesn't exist\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    # Loop through every row to get the info\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"REG\"]  # Separate reg nums\n",
    "        \n",
    "        # Split image URLs by commas for both normal and damaged images\n",
    "        image_urls = row[\"Images\"].split(\", \") if pd.notna(row[\"Images\"]) else []\n",
    "        damaged_urls = row[\"Damaged_image\"].split(\", \") if pd.notna(row[\"Damaged_image\"]) else []\n",
    "        \n",
    "        # Create a subfolder for the car registration number\n",
    "        reg_folder = os.path.join(main_folder, reg_no)  # Combine the main folder name and the reg num like Image/reg_no\n",
    "        os.makedirs(reg_folder, exist_ok=True)  # Create the subfolder\n",
    "        \n",
    "        # Create a subfolder for images inside the registration folder\n",
    "        images_folder = os.path.join(reg_folder, \"Original_images\")  # Folder for regular images\n",
    "        os.makedirs(images_folder, exist_ok=True)  # Ensure the \"Images\" folder exists\n",
    "\n",
    "        # Create a subfolder for damaged images inside the registration folder\n",
    "        damaged_folder = os.path.join(reg_folder, \"Damaged_images\")  # Folder for damaged images\n",
    "        os.makedirs(damaged_folder, exist_ok=True)  # Ensure the \"Damaged\" folder exists\n",
    "\n",
    "        # Download regular images\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip()  # Remove extra spaces\n",
    "            if not url.startswith((\"http://\", \"https://\")):  # Check if the URL starts with 'http' or 'https'\n",
    "                url = urljoin(\"https://\", url)  # Add 'https://' if missing\n",
    "\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(url)\n",
    "\n",
    "            # Check if the parsed URL is valid\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "            \n",
    "            # Try downloading the image\n",
    "            try:\n",
    "                # Download the image\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()  # Raises an exception if the HTTP request fails\n",
    "\n",
    "                # Extract the file name from the URL\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                # Check for valid file extensions\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\"  # Assign a default name if extension is invalid\n",
    "                \n",
    "                # Construct the full path for the file inside the \"Images\" folder\n",
    "                full_file_name = os.path.join(images_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "                \n",
    "                # Save the image\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "        # Download damaged images\n",
    "        for idx, url in enumerate(damaged_urls):\n",
    "            url = url.strip()  # Remove extra spaces\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)  # Add 'https://' if missing\n",
    "\n",
    "            # Parse the URL\n",
    "            parsed_url = urlparse(url)\n",
    "\n",
    "            # Check if the parsed URL is valid\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "            \n",
    "            # Try downloading the image\n",
    "            try:\n",
    "                # Download the image\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()  # Raises an exception if the HTTP request fails\n",
    "\n",
    "                # Extract the file name from the URL\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"damaged_{idx + 1}.jpg\"\n",
    "\n",
    "                # Check for valid file extensions\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"damaged_{idx + 1}.jpg\"  # Assign a default name if extension is invalid\n",
    "                \n",
    "                # Construct the full path for the file inside the \"Damaged\" folder\n",
    "                full_file_name = os.path.join(damaged_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "                \n",
    "                # Save the image\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "# Assuming `reg_img` contains the image data with \"REG\", \"Images\", and \"Damaged_image\"\n",
    "download_images(reg_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib.parse import urlparse, urljoin\n",
    "# # urlparse: Parses a URL into components (scheme, netloc, path, etc.), making it easy to validate or extract information from the URL.\n",
    "# # urljoin: Joins a base URL with a relative path to form a complete URL.\n",
    "\n",
    "# def download_images(data, main_folder=\"Images\"): # here the main folder is Images\n",
    "    \n",
    "#     # Create the main folder if it doesn't exist\n",
    "#     os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "#     # loop around every row to get the info\n",
    "#     for index, row in data.iterrows():\n",
    "#         reg_no = row[\"REG\"] # separate reg nums\n",
    "#         image_urls = row[\"Images\"].split(\", \")  # Split image URLs by comma if multiple\n",
    "        \n",
    "#         # Create a subfolder for the car registration number\n",
    "#         reg_folder = os.path.join(main_folder, reg_no) # combine the main folder name and the reg num like Image/reg_no\n",
    "#         os.makedirs(reg_folder, exist_ok=True) # create the subfolder of the type mention in the above line of code\n",
    "        \n",
    "#         # loop around the urls of the current row and also save index for further use\n",
    "#         for idx, url in enumerate(image_urls):\n",
    "#             url = url.strip()  # Remove extra spaces\n",
    "#             if not url.startswith((\"http://\", \"https://\")): # check if the url does not start with the values in the bracket\n",
    "#                 url = urljoin(\"https://\", url) # sets the urls starting from https://.....\n",
    "            \n",
    "#             # parse the url\n",
    "#             parsed_url = urlparse(url)\n",
    "\n",
    "#             # check if the parsed url is valid\n",
    "#             if not parsed_url.scheme or not parsed_url.netloc:\n",
    "#                 print(f\"Invalid URL skipped: {url}\") # incase some urls are incorret and are not loaded for downloading \n",
    "#                 continue\n",
    "            \n",
    "#             # try downloading images\n",
    "#             try:\n",
    "#                 # Download the image\n",
    "#                 response = requests.get(url, stream=True) # send the url for downloading\n",
    "#                 response.raise_for_status() # Raises an exception if the HTTP request fails (e.g., 404 or 500).\n",
    "                \n",
    "#                 # Extracts the file name from the URL path (e.g., image.jpg from http://example.com/image.jpg).\n",
    "#                 # If no file name is found, assigns a default name based on the index.\n",
    "#                 file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "#                 # Extracts the file extension (e.g., jpg). from the last index value\n",
    "#                 file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "#                 # Ensure the file has a valid extension\n",
    "#                 if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "#                     file_name = f\"image_{idx + 1}.jpg\" # set the extension if the extension is not in the above list\n",
    "                \n",
    "#                 # Construct the full path for the file\n",
    "#                 full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "                \n",
    "#                 # Save the image\n",
    "#                 with open(full_file_name, 'wb') as f:\n",
    "#                     for chunk in response.iter_content(1024): # Reads the response in 1KB chunks to save memory.\n",
    "#                         f.write(chunk)\n",
    "                \n",
    "#                 print(f\"Downloaded: {full_file_name}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "# # Call the function\n",
    "# download_images(reg_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
